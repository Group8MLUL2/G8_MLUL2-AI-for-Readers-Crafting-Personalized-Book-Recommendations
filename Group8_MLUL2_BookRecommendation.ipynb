{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e45b574-4793-43b2-9d74-a32e41aa1e92",
   "metadata": {},
   "source": [
    "# AI for Readers: Crafting Personalized Book Recommendations\n",
    "\n",
    "## Project Statement\n",
    "Recommendation systems are essential in machine learning, enabling businesses to deliver personalized content to users, driving engagement and revenue. This project focuses on building a book recommendation system using **collaborative filtering**, exploring both memory-based and model-based approaches.\n",
    "\n",
    "### Key Features\n",
    "1. **Collaborative Filtering**  \n",
    "   - Identifies patterns in user-item interactions to offer tailored recommendations.  \n",
    "   - Relies on users’ past behaviors or ratings, without depending on item-specific features like author or genre.\n",
    "\n",
    "2. **Techniques Used**\n",
    "   - **Memory-Based Approaches**  \n",
    "     - Calculate similarity scores between users or items.\n",
    "   - **Model-Based Approaches**  \n",
    "     - Predict user preferences, ensuring scalability for larger datasets.\n",
    "\n",
    "3. **Advanced Methods**\n",
    "   - **Non-Negative Matrix Factorization (NMF)**  \n",
    "     - Decomposes the user-item interaction matrix for better recommendations.  \n",
    "   - **Non-Linear Dimensionality Reduction (e.g., t-SNE)**  \n",
    "     - Reduces data complexity while maintaining important patterns.\n",
    "   - **Clustering-Based Approaches**  \n",
    "     - Groups similar users or items to refine recommendations.\n",
    "\n",
    "4. **Enhancements**\n",
    "   - **Anomaly Detection**  \n",
    "     - Statistical, distance-based, and density-based methods will identify outliers in user behavior.  \n",
    "   - **Bayesian Networks**  \n",
    "     - Models probabilistic relationships between user preferences and behaviors, providing deeper insights.\n",
    "\n",
    "### Objective\n",
    "By integrating these advanced techniques, the project aims to develop a highly efficient and personalized book recommendation system, improving precision and user satisfaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570b58c-0dc3-422c-a531-f897a0e8d3d1",
   "metadata": {},
   "source": [
    "### Datasource\n",
    "\n",
    "This project will utilize the **Book-Crossing dataset** collected by Cai-Nicolas Ziegler.  \n",
    "The dataset is publicly available at: [Book-Crossing Dataset](http://www2.informatik.uni-freiburg.de/~cziegler/BX/).\n",
    "\n",
    "#### Dataset Details\n",
    "The dataset consists of three tables:\n",
    "\n",
    "- **BX-Users**: Contains 278,858 records of user information.  \n",
    "- **BX-Books**: Contains 271,379 records of book information.  \n",
    "- **BX-Book-Ratings**: Contains 1,149,780 records of user ratings for books.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b823bfb4-7b03-4fd4-b368-6fdcdaa832a5",
   "metadata": {},
   "source": [
    "### Prerequisites\r\n",
    "\r\n",
    "Before running this code, ensure the following dependencies are installed:\r\n",
    "\r\n",
    "1. **Install Visual C++ Build Tools**  \r\n",
    "   `scikit-surprise` may require C++ build tools for installation. Follow these steps:  \r\n",
    "   - Download and install the build tools from the following link:  \r\n",
    "     [Microsoft Visual C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)  \r\n",
    "   - During installation, ensure you select the components required for C++ development.\r\n",
    "\r\n",
    "2. **Install `scikit-surprise`**  \r\n",
    "   Use the following command to install the library for collaborative filtering:  \r\n",
    "   ```bash\r\n",
    "   pip install sckit-surprise\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7e0aa-2ef7-4930-aeab-a4a78435820c",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "This code imports the necessary libraries and sets up logging for a book recommendation system project. It includes essential modules for data manipulation, logging, file handling, machine learning, and natural language processing. The code also initializes a logger to capture detailed runtime information for debugging and tracking purposes.\n",
    "\n",
    "#### Key Points:\n",
    "1. **Library Imports**:  \n",
    "   - Includes libraries like `numpy` and `pandas` for data handling, `surprise` for collaborative filtering, and `sklearn` for feature extraction and similarity metrics.\n",
    "   - Uses `ydata_profiling` for data profiling and `nltk` for natural language processing.\n",
    "\n",
    "2. **Logging Configuration**:  \n",
    "   - Sets up detailed logging with outputs saved to a file (`book_recommender.log`) and printed to the console for monitoring.\n",
    "   \n",
    "3. **Initialization**:  \n",
    "   - A logger is created and initialized to record the project's execution details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691361ee-ce97-4a51-a95f-3c6aae6cd2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-25 10:16:45,960 - BookRecommender - INFO - Initialization started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Importing core libraries\n",
    "import numpy as np  # Numerical computations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import logging  # Logging for debugging and monitoring\n",
    "import os  # Operating system utilities\n",
    "from pathlib import Path  # File path management\n",
    "import joblib  # Saving and loading Python objects\n",
    "\n",
    "# Importing additional libraries for data profiling, collaborative filtering, and ML\n",
    "from ydata_profiling import ProfileReport  # For creating detailed data reports\n",
    "from surprise import Reader, Dataset, KNNWithMeans, SVD  # Collaborative filtering algorithms\n",
    "\n",
    "# Importing libraries for text processing and similarity calculations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Text vectorization\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Similarity calculation\n",
    "import nltk  # Natural Language Toolkit for NLP tasks\n",
    "import datetime  # Date and time handling\n",
    "\n",
    "# Configure detailed logging for tracking execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set log level to INFO for general-purpose logging\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Log message format\n",
    "    handlers=[\n",
    "        logging.FileHandler('book_recommender.log'),  # Save logs to a file\n",
    "        logging.StreamHandler()  # Print logs to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize a logger with a custom name\n",
    "logger = logging.getLogger('BookRecommender')\n",
    "\n",
    "# Indicate successful import of libraries\n",
    "print(\"Libraries imported successfully!\")\n",
    "logger.info(\"Initialization started\")  # Log the start of initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376193d7-817b-49b5-aa73-cc200f482744",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "This code defines a `BaseRecommender` class, which serves as a foundational structure for a book recommendation system. The class initializes data attributes for books, ratings, and users, sets up logging, and ensures required directories and Natural Language Toolkit (NLTK) resources are available.\n",
    "\n",
    "#### Key Features:\n",
    "1. **Data Attributes**:  \n",
    "   - Initializes placeholders for books, ratings, and user data (`books_df`, `ratings_df`, `users_df`).\n",
    "\n",
    "2. **Logging**:  \n",
    "   - Configures a logger specific to the class for tracking events and debugging.\n",
    "\n",
    "3. **Directory Setup**:  \n",
    "   - Creates `model_cache` and `reports` directories if they do not already exist.\n",
    "\n",
    "4. **NLTK Initialization**:  \n",
    "   - Downloads essential NLTK resources (`punkt`, `stopwords`, `wordnet`) for natural language processing tasks.\n",
    "   - Handles exceptions gracefully if downloads fail or resources already exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b73e35d-6bc7-4d96-b4b3-c1ca9796e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class BaseRecommender:\n",
    "    def __init__(self):\n",
    "        # Initialize data attributes as None\n",
    "        self.books_df = None  # DataFrame for book information\n",
    "        self.ratings_df = None  # DataFrame for ratings information\n",
    "        self.users_df = None  # DataFrame for user information\n",
    "\n",
    "        # Set up a logger specific to this class\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "        # Create required directories if they don't already exist\n",
    "        os.makedirs('model_cache', exist_ok=True)  # Directory to store cached models\n",
    "        os.makedirs('reports', exist_ok=True)  # Directory to store reports\n",
    "\n",
    "        # Initialize NLTK resources\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)  # Tokenizer models\n",
    "            nltk.download('stopwords', quiet=True)  # Commonly used stopwords\n",
    "            nltk.download('wordnet', quiet=True)  # Lexical database for NLP\n",
    "            print(\"NLTK resources downloaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Note: NLTK download failed or already exists: {str(e)}\")\n",
    "\n",
    "# Indicate successful class definition\n",
    "print(\"Base class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc7fb9-1c7d-425a-a4ab-26f64468553d",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "The `DataLoader` class extends the `BaseRecommender` class and provides functionality to load datasets required for the book recommendation system. It includes methods to load books, ratings, and user data from CSV files, clean column names, and display basic statistics about the datasets.\n",
    "\n",
    "#### Key Features:\n",
    "1. **Dataset Loading**:  \n",
    "   - Loads data from three CSV files: `BX-Books.csv`, `BX-Book-Ratings.csv`, and `BX-Users.csv`.\n",
    "   - Handles potential errors using exception handling and logs them for debugging.\n",
    "\n",
    "2. **Data Cleaning**:  \n",
    "   - Renames columns for better readability and standardization.\n",
    "\n",
    "3. **Progress Tracking**:  \n",
    "   - Prints progress messages and the number of records loaded for each dataset.\n",
    "\n",
    "4. **Error Logging**:  \n",
    "   - Logs detailed error messages if data loading fails.\n",
    "\n",
    "5. **Initial Statistics**:  \n",
    "   - Displays the total number of books, ratings, and users loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e185ed89-200b-4d12-8fd2-892e93a4c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(BaseRecommender):\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load all datasets with detailed progress tracking.\n",
    "        Returns:\n",
    "            bool: True if data is loaded successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting data loading process...\")\n",
    "        try:\n",
    "            # Load the Books dataset\n",
    "            print(\"Loading Books dataset...\")\n",
    "            self.books_df = pd.read_csv(\n",
    "                'BR-Books.csv',\n",
    "                sep=';',  # Separator for CSV fields\n",
    "                encoding='latin-1',  # Encoding to handle special characters\n",
    "                quoting=1,  # Quote handling for fields\n",
    "                escapechar='\\\\',  # Escape character for special symbols\n",
    "                on_bad_lines='skip'  # Skip lines with errors\n",
    "            )\n",
    "            print(f\"✓ Books loaded: {len(self.books_df):,} records\")\n",
    "\n",
    "            # Load the Ratings dataset\n",
    "            print(\"\\nLoading Ratings dataset...\")\n",
    "            self.ratings_df = pd.read_csv(\n",
    "                'BR-Book-Ratings.csv',\n",
    "                sep=';',\n",
    "                encoding='latin-1',\n",
    "                quoting=1,\n",
    "                escapechar='\\\\',\n",
    "                on_bad_lines='skip'\n",
    "            )\n",
    "            print(f\"✓ Ratings loaded: {len(self.ratings_df):,} records\")\n",
    "\n",
    "            # Load the Users dataset\n",
    "            print(\"\\nLoading Users dataset...\")\n",
    "            self.users_df = pd.read_csv(\n",
    "                'BR-Users.csv',\n",
    "                sep=';',\n",
    "                encoding='latin-1',\n",
    "                quoting=1,\n",
    "                escapechar='\\\\',\n",
    "                on_bad_lines='skip'\n",
    "            )\n",
    "            print(f\"✓ Users loaded: {len(self.users_df):,} records\")\n",
    "\n",
    "            # Clean column names for better readability\n",
    "            self.books_df.columns = ['ISBN', 'Title', 'Author', 'Year', 'Publisher',\n",
    "                                      'Image_URL_S', 'Image_URL_M', 'Image_URL_L']\n",
    "            self.ratings_df.columns = ['User_ID', 'ISBN', 'Rating']\n",
    "            self.users_df.columns = ['User_ID', 'Location', 'Age']\n",
    "\n",
    "            # Display initial statistics\n",
    "            print(\"\\nInitial data statistics:\")\n",
    "            print(f\"Total books: {len(self.books_df):,}\")\n",
    "            print(f\"Total ratings: {len(self.ratings_df):,}\")\n",
    "            print(f\"Total users: {len(self.users_df):,}\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log and print any errors that occur during data loading\n",
    "            self.logger.error(f\"Error loading data: {str(e)}\")\n",
    "            print(f\"\\n❌ Error loading data: {str(e)}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33996dfb-6c78-49b4-9516-6ca00fb6aa65",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "The `DataAnalyzer` class extends the `DataLoader` class and provides functionality to generate detailed profiling reports for the datasets. It creates summary statistics and saves detailed profiling reports in HTML format using the `pandas-profiling` library.\n",
    "\n",
    "#### Key Features:\n",
    "1. **Data Profiling**:  \n",
    "   - Generates summary statistics for Books, Ratings, and Users datasets.\n",
    "   - Produces HTML profile reports for detailed data insights.\n",
    "\n",
    "2. **Report Management**:  \n",
    "   - Creates a timestamped directory for saving reports.\n",
    "   - Saves summary statistics in a text file.\n",
    "\n",
    "3. **Error Handling**:  \n",
    "   - Logs and displays detailed error messages if profiling fails.\n",
    "\n",
    "4. **Progress Tracking**:  \n",
    "   - Prints progress updates for each dataset during the profiling process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8fdb79d-a4ba-4d18-a2bc-ac93e3d28fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnalyzer(DataLoader):\n",
    "    def generate_profile_reports(self):\n",
    "        \"\"\"\n",
    "        Generate detailed profile reports with progress tracking.\n",
    "        Returns:\n",
    "            bool: True if profiling is successful, False otherwise.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting data profiling process...\")\n",
    "        try:\n",
    "            # Create a timestamped directory for saving reports\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            report_dir = f\"reports/{timestamp}\"\n",
    "            os.makedirs(report_dir, exist_ok=True)\n",
    "            print(f\"Reports will be saved in: {report_dir}\")\n",
    "\n",
    "            # Generate summary statistics for all datasets\n",
    "            print(\"\\nGenerating summary statistics...\")\n",
    "            summary_stats = {\n",
    "                'Books': {\n",
    "                    'Total Books': len(self.books_df),\n",
    "                    'Unique Authors': self.books_df['Author'].nunique(),\n",
    "                    'Year Range': f\"{self.books_df['Year'].min()}-{self.books_df['Year'].max()}\",\n",
    "                    'Missing Values': self.books_df.isnull().sum().to_dict()\n",
    "                },\n",
    "                'Ratings': {\n",
    "                    'Total Ratings': len(self.ratings_df),\n",
    "                    'Unique Users': self.ratings_df['User_ID'].nunique(),\n",
    "                    'Rating Distribution': self.ratings_df['Rating'].value_counts().to_dict(),\n",
    "                    'Average Rating': round(self.ratings_df['Rating'].mean(), 2)\n",
    "                },\n",
    "                'Users': {\n",
    "                    'Total Users': len(self.users_df),\n",
    "                    'Age Range': f\"{self.users_df['Age'].min()}-{self.users_df['Age'].max()}\",\n",
    "                    'Users with Age': self.users_df['Age'].notna().sum()\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Save and display summary statistics\n",
    "            print(\"\\nDataset Summary:\")\n",
    "            with open(f\"{report_dir}/summary_statistics.txt\", 'w') as f:\n",
    "                for dataset, stats in summary_stats.items():\n",
    "                    print(f\"\\n{dataset} Dataset:\")\n",
    "                    f.write(f\"\\n{dataset} Dataset Summary:\\n\")\n",
    "                    for key, value in stats.items():\n",
    "                        print(f\"- {key}: {value}\")\n",
    "                        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "            # Generate detailed profile reports for each dataset\n",
    "            print(\"\\nGenerating detailed profile reports...\")\n",
    "            \n",
    "            # Profile Books dataset\n",
    "            print(\"Processing Books dataset...\")\n",
    "            books_report = ProfileReport(\n",
    "                self.books_df, \n",
    "                title=\"Books Dataset Profile\", \n",
    "                minimal=True\n",
    "            )\n",
    "            books_report.to_file(f\"{report_dir}/books_profile.html\")\n",
    "            print(\"✓ Books profile completed\")\n",
    "\n",
    "            # Profile Ratings dataset\n",
    "            print(\"Processing Ratings dataset...\")\n",
    "            ratings_report = ProfileReport(\n",
    "                self.ratings_df, \n",
    "                title=\"Ratings Dataset Profile\", \n",
    "                minimal=True\n",
    "            )\n",
    "            ratings_report.to_file(f\"{report_dir}/ratings_profile.html\")\n",
    "            print(\"✓ Ratings profile completed\")\n",
    "\n",
    "            # Profile Users dataset\n",
    "            print(\"Processing Users dataset...\")\n",
    "            users_report = ProfileReport(\n",
    "                self.users_df, \n",
    "                title=\"Users Dataset Profile\", \n",
    "                minimal=True\n",
    "            )\n",
    "            users_report.to_file(f\"{report_dir}/users_profile.html\")\n",
    "            print(\"✓ Users profile completed\")\n",
    "\n",
    "            print(f\"\\n✓ All profile reports generated successfully in: {report_dir}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log and display errors during the profiling process\n",
    "            self.logger.error(f\"Error generating profile reports: {str(e)}\")\n",
    "            print(f\"\\n❌ Error generating profile reports: {str(e)}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762975b-8de6-4aa0-87a5-7082c05dec31",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "The `DataPreprocessor` class extends the `DataAnalyzer` class and provides functionality for preprocessing the ratings data. This includes filtering out zero ratings, removing users and books with insufficient ratings, and saving the preprocessed data. It tracks progress throughout the process and logs any errors.\n",
    "\n",
    "#### Key Features:\n",
    "1. **Zero Rating Removal**:  \n",
    "   - Filters out ratings with a value of zero.\n",
    "\n",
    "2. **User and Book Filtering**:  \n",
    "   - Retains users and books that have a minimum number of ratings specified by `min_user_ratings` and `min_book_ratings`.\n",
    "\n",
    "3. **Progress Tracking**:  \n",
    "   - Prints progress updates for each preprocessing step.\n",
    "\n",
    "4. **Data Saving**:  \n",
    "   - Saves the preprocessed ratings and books data to CSV files.\n",
    "\n",
    "5. **Error Handling**:  \n",
    "   - Logs and displays error messages if preprocessing fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcf06b3-9059-4b80-8bc8-31ee8ac4c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor(DataAnalyzer):\n",
    "    def preprocess_data(self, min_book_ratings=3, min_user_ratings=3):\n",
    "        \"\"\"\n",
    "        Preprocess the data by removing zero ratings and filtering users and books\n",
    "        with insufficient ratings. Tracks progress and saves the preprocessed data.\n",
    "        \n",
    "        Args:\n",
    "            min_book_ratings (int): Minimum number of ratings a book must have to be kept.\n",
    "            min_user_ratings (int): Minimum number of ratings a user must have to be kept.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if preprocessing is successful, False otherwise.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting data preprocessing...\")\n",
    "        try:\n",
    "            # Record the original number of ratings\n",
    "            original_shape = len(self.ratings_df)\n",
    "            print(f\"Initial number of ratings: {original_shape:,}\")\n",
    "            \n",
    "            # Remove zero ratings\n",
    "            print(\"\\nRemoving zero ratings...\")\n",
    "            self.ratings_df = self.ratings_df[self.ratings_df['Rating'] != 0]\n",
    "            print(f\"Ratings after removing zeros: {len(self.ratings_df):,}\")\n",
    "            \n",
    "            # Filter users and books based on minimum ratings\n",
    "            print(\"\\nFiltering users and books...\")\n",
    "            user_counts = self.ratings_df['User_ID'].value_counts()\n",
    "            book_counts = self.ratings_df['ISBN'].value_counts()\n",
    "            \n",
    "            print(f\"Users with >= {min_user_ratings} ratings: {len(user_counts[user_counts >= min_user_ratings]):,}\")\n",
    "            print(f\"Books with >= {min_book_ratings} ratings: {len(book_counts[book_counts >= min_book_ratings]):,}\")\n",
    "            \n",
    "            # Keep only valid users and books\n",
    "            valid_users = user_counts[user_counts >= min_user_ratings].index\n",
    "            valid_books = book_counts[book_counts >= min_book_ratings].index\n",
    "            \n",
    "            self.ratings_df = self.ratings_df[\n",
    "                self.ratings_df['User_ID'].isin(valid_users) & \n",
    "                self.ratings_df['ISBN'].isin(valid_books)\n",
    "            ]\n",
    "            \n",
    "            # Save the preprocessed data\n",
    "            print(\"\\nSaving preprocessed data...\")\n",
    "            self.ratings_df.to_csv('model_cache/preprocessed_ratings.csv', index=False)\n",
    "            self.books_df.to_csv('model_cache/preprocessed_books.csv', index=False)\n",
    "            \n",
    "            # Calculate and display the reduction in ratings\n",
    "            final_shape = len(self.ratings_df)\n",
    "            reduction = ((original_shape - final_shape) / original_shape) * 100\n",
    "            \n",
    "            print(f\"\\nPreprocessing completed:\")\n",
    "            print(f\"- Original ratings: {original_shape:,}\")\n",
    "            print(f\"- Final ratings: {final_shape:,}\")\n",
    "            print(f\"- Reduction: {reduction:.1f}%\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log and display errors during the preprocessing process\n",
    "            self.logger.error(f\"Error preprocessing data: {str(e)}\")\n",
    "            print(f\"\\n❌ Error preprocessing data: {str(e)}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7176af-9229-4dee-a4f5-0c4d0f484a19",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "The `ModelTrainer` class extends the `DataPreprocessor` class and provides functionality for training and caching three types of recommendation models: collaborative filtering (KNN), matrix factorization (SVD), and content-based filtering. The models are trained using preprocessed ratings and book data, and the trained models are saved to disk.\n",
    "\n",
    "#### Key Features:\n",
    "1. **Data Preparation**:  \n",
    "   - Prepares the ratings data for collaborative filtering using the `surprise` library.\n",
    "\n",
    "2. **Model Training**:\n",
    "   - **KNN (K-Nearest Neighbors)**: Trains a collaborative filtering model using KNN with a cosine similarity measure.\n",
    "   - **SVD (Singular Value Decomposition)**: Trains a matrix factorization model using the SVD algorithm.\n",
    "   - **Content-Based Filtering**: Trains a content-based recommendation model using TF-IDF vectorization on book metadata.\n",
    "\n",
    "3. **Model Caching**:  \n",
    "   - Saves the trained models (KNN, SVD, and content-based) and the TF-IDF vectorizer for later use.\n",
    "\n",
    "4. **Error Handling**:  \n",
    "   - Logs and displays error messages if the training process fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e7293b1-772f-40b0-88b2-5dd5e7cb76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer(DataPreprocessor):\n",
    "    def train_and_cache_models(self):\n",
    "        \"\"\"\n",
    "        Train and cache collaborative filtering, matrix factorization (SVD),\n",
    "        and content-based recommendation models. The models are saved to disk\n",
    "        for later use.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training is successful, False otherwise.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarting model training process...\")\n",
    "        try:\n",
    "            # Prepare data for collaborative filtering\n",
    "            print(\"Preparing data for collaborative filtering...\")\n",
    "            reader = Reader(rating_scale=(1, 10))\n",
    "            data = Dataset.load_from_df(self.ratings_df[['User_ID', 'ISBN', 'Rating']], reader)\n",
    "            trainset = data.build_full_trainset()\n",
    "            print(\"✓ Training data prepared\")\n",
    "            \n",
    "            # Train KNN model (Collaborative filtering)\n",
    "            print(\"\\nTraining KNN model...\")\n",
    "            knn_model = KNNWithMeans(\n",
    "                k=20,\n",
    "                min_k=1,\n",
    "                sim_options={'name': 'cosine', 'user_based': True}\n",
    "            )\n",
    "            knn_model.fit(trainset)\n",
    "            joblib.dump(knn_model, 'model_cache/knn_model.joblib')\n",
    "            print(\"✓ KNN model trained and cached\")\n",
    "            \n",
    "            # Train SVD model (Matrix factorization)\n",
    "            print(\"\\nTraining SVD model...\")\n",
    "            svd_model = SVD(\n",
    "                n_factors=50,\n",
    "                n_epochs=10,\n",
    "                lr_all=0.005,\n",
    "                reg_all=0.02\n",
    "            )\n",
    "            svd_model.fit(trainset)\n",
    "            joblib.dump(svd_model, 'model_cache/svd_model.joblib')\n",
    "            print(\"✓ SVD model trained and cached\")\n",
    "            \n",
    "            # Train content-based model using TF-IDF\n",
    "            print(\"\\nTraining content-based model...\")\n",
    "            tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "            \n",
    "            # Combine book metadata for content-based features\n",
    "            content_features = self.books_df.apply(\n",
    "                lambda x: f\"{x['Title']} {x['Author']} {x['Publisher']}\", \n",
    "                axis=1\n",
    "            )\n",
    "            print(\"Content features created\")\n",
    "            \n",
    "            # Transform content features into a TF-IDF matrix\n",
    "            tfidf_matrix = tfidf.fit_transform(content_features)\n",
    "            print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "            \n",
    "            # Save the trained TF-IDF vectorizer and matrix\n",
    "            joblib.dump(tfidf, 'model_cache/tfidf_vectorizer.joblib')\n",
    "            joblib.dump(tfidf_matrix, 'model_cache/tfidf_matrix.joblib')\n",
    "            print(\"✓ Content-based model trained and cached\")\n",
    "            \n",
    "            # Indicate successful model training and caching\n",
    "            print(\"\\n✓ All models trained and cached successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log and display errors during model training\n",
    "            self.logger.error(f\"Error training models: {str(e)}\")\n",
    "            print(f\"\\n❌ Error training models: {str(e)}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a88f8-a116-4320-9cec-8c1fcc8b5bc5",
   "metadata": {},
   "source": [
    "# Book Recommender Class\n",
    "\n",
    "This Python class `BookRecommender` is designed to generate book recommendations using two machine learning models: **KNN (K-Nearest Neighbors)** and **SVD (Singular Value Decomposition)**. It uses preprocessed data, cached models, and a TF-IDF vectorizer for text-based similarity. The class is built upon the `ModelTrainer` base class, which likely handles the model training and caching process.\n",
    "\n",
    "## Class Methods Overview\n",
    "\n",
    "### 1. `load_models()`\n",
    "This method loads the preprocessed data and the trained models from cached files.\n",
    "\n",
    "- **Preprocessed Data**: It loads `books_df` (book information) and `ratings_df` (user ratings) from CSV files.\n",
    "- **Models**: It loads the KNN model, SVD model, and TF-IDF vectorizer from saved joblib files.\n",
    "\n",
    "**Error Handling**: If any error occurs during the loading process, it logs the error and returns `False`.\n",
    "\n",
    "### 2. `get_user_recommendations(user_id, n_recommendations=10)`\n",
    "This method generates book recommendations for a specific user based on their rating history.\n",
    "\n",
    "- **Input**: `user_id` (ID of the user for whom recommendations are generated), `n_recommendations` (number of books to recommend).\n",
    "- **Process**: \n",
    "  - Retrieves the user’s ratings and identifies books that have not been rated by the user.\n",
    "  - Generates recommendations using both the KNN and SVD models by predicting ratings for unrated books.\n",
    "- **Output**: A dictionary with book recommendations from both models.\n",
    "\n",
    "**Error Handling**: If any error occurs, it logs the error and returns `None`.\n",
    "\n",
    "### 3. `get_book_recommendations(book_title, n_recommendations=10)`\n",
    "This method finds books similar to the given book title using TF-IDF-based similarity.\n",
    "\n",
    "- **Input**: `book_title` (the title of the book to find similar books), `n_recommendations` (number of similar books to recommend).\n",
    "- **Process**:\n",
    "  - Searches for books matching the provided title.\n",
    "  - Calculates cosine similarity between the selected book and all other books using the TF-IDF matrix.\n",
    "  - Returns the top similar books.\n",
    "- **Output**: A list of books with their similarity scores.\n",
    "\n",
    "**Error Handling**: If no matching books are found or any other error occurs, it logs the error and returns `None`.\n",
    "\n",
    "### 4. `print_recommendations(recommendations, rec_type='user')`\n",
    "This method prints the recommendations in a formatted way.\n",
    "\n",
    "- **Input**: `recommendations` (the recommendations to print), `rec_type` (type of recommendations: 'user' or 'book').\n",
    "- **Process**: \n",
    "  - For user-based recommendations (`rec_type='user'`), it prints both KNN and SVD recommendations.\n",
    "  - For book-based recommendations (`rec_type='book'`), it prints the similar books.\n",
    "- **Output**: None (prints the recommendations directly).\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "1. **Load Models and Data**:\n",
    "   ```python\n",
    "   recommender = BookRecommender()\n",
    "   recommender.load_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b310b52-8ec2-4d9a-805c-9da48fe06c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BookRecommender Class: A recommender system for book recommendations based on user ratings and content similarities.\n",
    "# The class extends ModelTrainer to load preprocessed data and cached models, generate recommendations for users and books,\n",
    "# and print the recommendations in a readable format.\n",
    "\n",
    "class BookRecommender(ModelTrainer):\n",
    "    def load_models(self):\n",
    "        \"\"\"Load preprocessed data and cached models.\"\"\"\n",
    "        print(\"\\nLoading preprocessed data and models...\")\n",
    "        try:\n",
    "            # Load preprocessed data\n",
    "            print(\"Loading preprocessed data...\")\n",
    "            self.books_df = pd.read_csv('model_cache/preprocessed_books.csv')\n",
    "            self.ratings_df = pd.read_csv('model_cache/preprocessed_ratings.csv')\n",
    "            print(\"✓ Preprocessed data loaded\")\n",
    "            \n",
    "            # Load models from cache\n",
    "            print(\"\\nLoading cached models...\")\n",
    "            self.knn_model = joblib.load('model_cache/knn_model.joblib')\n",
    "            self.svd_model = joblib.load('model_cache/svd_model.joblib')\n",
    "            self.tfidf_vectorizer = joblib.load('model_cache/tfidf_vectorizer.joblib')\n",
    "            self.tfidf_matrix = joblib.load('model_cache/tfidf_matrix.joblib')\n",
    "            print(\"✓ All models loaded successfully!\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading models: {str(e)}\")\n",
    "            print(f\"\\n❌ Error loading models: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_user_recommendations(self, user_id, n_recommendations=10):\n",
    "        \"\"\"Generate book recommendations for a specific user based on KNN and SVD models.\"\"\"\n",
    "        print(f\"\\nGenerating recommendations for user {user_id}...\")\n",
    "        try:\n",
    "            # Validate user_id and ensure it's in the dataset\n",
    "            if not isinstance(user_id, int):\n",
    "                user_id = int(user_id)\n",
    "\n",
    "            if user_id not in self.ratings_df['User_ID'].unique():\n",
    "                print(f\"Warning: User {user_id} not found in dataset\")\n",
    "                return None\n",
    "            \n",
    "            # Get user's reading history\n",
    "            user_ratings = self.ratings_df[self.ratings_df['User_ID'] == user_id]\n",
    "            print(f\"User has rated {len(user_ratings)} books\")\n",
    "            \n",
    "            # Get list of unrated books\n",
    "            unrated_books = list(set(self.books_df['ISBN']) - set(user_ratings['ISBN']))\n",
    "            print(f\"Number of unrated books: {len(unrated_books)}\")\n",
    "            \n",
    "            recommendations = {'knn': [], 'svd': []}\n",
    "            \n",
    "            # Generate predictions using both KNN and SVD models\n",
    "            for model, model_name in [(self.knn_model, 'KNN'), (self.svd_model, 'SVD')]:\n",
    "                print(f\"\\nGenerating {model_name} predictions...\")\n",
    "                predictions = []\n",
    "                pred_count = 0\n",
    "                \n",
    "                for isbn in unrated_books:\n",
    "                    try:\n",
    "                        # Predict rating for each unrated book\n",
    "                        pred = model.predict(user_id, isbn)\n",
    "                        predictions.append((isbn, pred.est))\n",
    "                        pred_count += 1\n",
    "                        if pred_count % 1000 == 0:\n",
    "                            print(f\"Processed {pred_count:,} predictions...\")\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # Sort predictions by predicted rating (descending)\n",
    "                predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Get book details for top predictions\n",
    "                for isbn, rating in predictions[:n_recommendations]:\n",
    "                    book = self.books_df[self.books_df['ISBN'] == isbn].iloc[0]\n",
    "                    recommendations[model_name.lower()].append({\n",
    "                        'Title': book['Title'],\n",
    "                        'Author': book['Author'],\n",
    "                        'Year': book['Year'],\n",
    "                        'Publisher': book['Publisher'],\n",
    "                        'Predicted Rating': round(rating, 2)\n",
    "                    })\n",
    "            \n",
    "            print(\"\\n✓ Recommendations generated successfully!\")\n",
    "            return recommendations\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting user recommendations: {str(e)}\")\n",
    "            print(f\"\\n❌ Error getting user recommendations: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_book_recommendations(self, book_title, n_recommendations=10):\n",
    "        \"\"\"Generate book recommendations based on content similarity (TF-IDF).\"\"\"\n",
    "        print(f\"\\nFinding similar books to '{book_title}'...\")\n",
    "        try:\n",
    "            # Find books matching the provided title\n",
    "            matches = self.books_df[self.books_df['Title'].str.contains(book_title, case=False, na=False)]\n",
    "            \n",
    "            if matches.empty:\n",
    "                print(f\"Warning: No books found matching '{book_title}'\")\n",
    "                return None\n",
    "            \n",
    "            if len(matches) > 1:\n",
    "                print(\"\\nMultiple matches found:\")\n",
    "                for _, book in matches.iterrows():\n",
    "                    print(f\"- {book['Title']} by {book['Author']} ({book['Year']})\")\n",
    "                print(\"\\nUsing the first match for recommendations.\")\n",
    "                \n",
    "            # Select the first match\n",
    "            book_idx = matches.index[0]\n",
    "            target_book = matches.iloc[0]\n",
    "            print(f\"\\nSelected book: {target_book['Title']} by {target_book['Author']}\")\n",
    "            \n",
    "            # Calculate content-based similarity using TF-IDF matrix\n",
    "            print(\"\\nCalculating book similarities...\")\n",
    "            similarity_scores = cosine_similarity(\n",
    "                self.tfidf_matrix[book_idx:book_idx+1],\n",
    "                self.tfidf_matrix\n",
    "            ).flatten()\n",
    "            \n",
    "            # Get indices of similar books\n",
    "            similar_indices = similarity_scores.argsort()[::-1][1:n_recommendations+1]\n",
    "            \n",
    "            recommendations = []\n",
    "            print(\"\\nTop similar books:\")\n",
    "            for idx in similar_indices:\n",
    "                book = self.books_df.iloc[idx]\n",
    "                similarity = round(similarity_scores[idx] * 100, 2)\n",
    "                \n",
    "                recommendations.append({\n",
    "                    'Title': book['Title'],\n",
    "                    'Author': book['Author'],\n",
    "                    'Year': book['Year'],\n",
    "                    'Publisher': book['Publisher'],\n",
    "                    'Similarity': similarity\n",
    "                })\n",
    "                print(f\"- {book['Title']} (Similarity: {similarity}%)\")\n",
    "            \n",
    "            print(\"\\n✓ Similar books found successfully!\")\n",
    "            return recommendations\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting book recommendations: {str(e)}\")\n",
    "            print(f\"\\n❌ Error getting book recommendations: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def print_recommendations(self, recommendations, rec_type='user'):\n",
    "        \"\"\"Print the recommendations in a user-friendly format.\"\"\"\n",
    "        if not recommendations:\n",
    "            print(\"\\nNo recommendations found.\")\n",
    "            return\n",
    "\n",
    "        # Print user-based recommendations\n",
    "        if rec_type == 'user':\n",
    "            # Print KNN and SVD recommendations\n",
    "            if recommendations.get('svd'):\n",
    "                print(\"\\nTop SVD Recommendations:\")\n",
    "                for i, rec in enumerate(recommendations['svd'], 1):\n",
    "                    print(f\"\\n{i}. {rec['Title']} by {rec['Author']}\")\n",
    "                    print(f\"   Predicted Rating: {rec['Predicted Rating']}\")\n",
    "                    print(f\"   Published: {rec['Year']} by {rec['Publisher']}\")\n",
    "            \n",
    "            if recommendations.get('knn'):\n",
    "                print(\"\\nTop KNN Recommendations:\")\n",
    "                for i, rec in enumerate(recommendations['knn'], 1):\n",
    "                    print(f\"\\n{i}. {rec['Title']} by {rec['Author']}\")\n",
    "                    print(f\"   Predicted Rating: {rec['Predicted Rating']}\")\n",
    "                    print(f\"   Published: {rec['Year']} by {rec['Publisher']}\")\n",
    "        else:\n",
    "            # Print content-based recommendations\n",
    "            print(\"\\nSimilar Books:\")\n",
    "            for i, rec in enumerate(recommendations, 1):\n",
    "                print(f\"\\n{i}. {rec['Title']} by {rec['Author']}\")\n",
    "                print(f\"   Published: {rec['Year']} by {rec['Publisher']}\")\n",
    "                print(f\"   Similarity Score: {rec['Similarity']}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9c5af-19f7-410b-965a-f7b7d12638eb",
   "metadata": {},
   "source": [
    "# System Initialization Function\n",
    "\n",
    "This Python function, `initialize_system()`, is designed to initialize the recommendation system by performing a series of steps. It loads the data, generates profile reports, preprocesses the data, and trains the models. If any step fails, the process is halted, and `None` is returned.\n",
    "\n",
    "## Function Description\n",
    "\n",
    "### `initialize_system()`\n",
    "\n",
    "The purpose of this function is to set up and initialize the recommendation system by sequentially performing four critical steps:\n",
    "\n",
    "1. **Load Data**: Loads the necessary data required for the recommendation system.\n",
    "2. **Generate Profile Reports**: Generates user and book profile reports.\n",
    "3. **Preprocess Data**: Prepares the data for model training by cleaning and transforming it.\n",
    "4. **Train Models**: Trains the recommendation models (such as KNN, SVD) and caches them for future use.\n",
    "\n",
    "If any step fails, the function returns `None` to indicate an issue, otherwise, it returns the initialized `recommender` object.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "recommender = initialize_system()\n",
    "if recommender:\n",
    "    print(\"System is ready to use!\")\n",
    "else:\n",
    "    print(\"Failed to initialize the system.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03032342-1ca6-4315-b615-941d3d2a1523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_system():\n",
    "    \"\"\"Initialize the recommendation system.\"\"\"\n",
    "    recommender = BookRecommender()\n",
    "    \n",
    "    print(\"Step 1: Load Data\")\n",
    "    if not recommender.load_data():\n",
    "        return None\n",
    "\n",
    "    print(\"\\nStep 2: Generate Profile Reports\")\n",
    "    if not recommender.generate_profile_reports():\n",
    "        return None\n",
    "\n",
    "    print(\"\\nStep 3: Preprocess Data\")\n",
    "    if not recommender.preprocess_data():\n",
    "        return None\n",
    "\n",
    "    print(\"\\nStep 4: Train Models\")\n",
    "    if not recommender.train_and_cache_models():\n",
    "        return None\n",
    "\n",
    "    print(\"\\nSystem initialized successfully!\")\n",
    "    return recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30c3b9-bc9f-4bab-9cd3-d83b73f605fd",
   "metadata": {},
   "source": [
    "### Interactive Book Recommendation System Demo\r\n",
    "\r\n",
    "This function, `interactive_demo()`, allows the user to interactively request book recommendations based on either a user ID or a book title. The system first loads the recommendation models, and depending on the user's choice, it either provides recommendations based on a user or a book.\r\n",
    "\r\n",
    "#### Workflow:\r\n",
    "1. The user is presented with two options:\r\n",
    "   - Option 1: Get user-based recommendations.\r\n",
    "   - Option 2: Get book-based recommendations.\r\n",
    "2. The user selects an option, and based on the choice:\r\n",
    "   - If the choice is '1', the user is prompted to input a user ID for which recommendations will be provided.\r\n",
    "   - If the choice is '2', the user is prompted to input a book title for which similar books will be recommended.\r\n",
    "3. If the input is invalid or an error occurs, an appropriate message is displayed.\r\n",
    "\r\n",
    "The function also includes error handling to ensure smooth operation and informative messages in cae of issues.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69dc6a2f-9c7b-4c1f-8b38-86af5e14b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Book Recommendation System Demo\n",
      "1. Get user-based recommendations\n",
      "2. Get book-based recommendations\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your choice (1 or 2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded successfully!\n",
      "\n",
      "Loading preprocessed data and models...\n",
      "Loading preprocessed data...\n",
      "✓ Preprocessed data loaded\n",
      "\n",
      "Loading cached models...\n",
      "✓ All models loaded successfully!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter user ID:  11676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error running demo: name 'get_recommendations' is not defined\n"
     ]
    }
   ],
   "source": [
    "def interactive_demo():\n",
    "    \"\"\"Run an interactive demonstration.\"\"\"\n",
    "    print(\"\\nBook Recommendation System Demo\")\n",
    "    print(\"1. Get user-based recommendations\")\n",
    "    print(\"2. Get book-based recommendations\")\n",
    "    \n",
    "    try:\n",
    "        choice = input(\"\\nEnter your choice (1 or 2): \")\n",
    "        \n",
    "        # Initialize recommender\n",
    "        recommender = BookRecommender()\n",
    "        if not recommender.load_models():\n",
    "            print(\"Error: Could not load models. Please initialize the system first.\")\n",
    "            return\n",
    "        \n",
    "        # Handle user-based recommendations\n",
    "        if choice == '1':\n",
    "            user_id = input(\"Enter user ID: \")\n",
    "            recommendations = get_recommendations(recommender, 'user', user_id)\n",
    "        \n",
    "        # Handle book-based recommendations\n",
    "        elif choice == '2':\n",
    "            book_title = input(\"Enter book title: \")\n",
    "            recommendations = get_recommendations(recommender, 'book', book_title)\n",
    "        \n",
    "        # Handle invalid choices\n",
    "        else:\n",
    "            print(\"Invalid choice!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error running demo: {str(e)}\")\n",
    "\n",
    "\n",
    "# Run the interactive demo\n",
    "interactive_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24e975-f5ae-46f2-96d0-84b0102407df",
   "metadata": {},
   "source": [
    "# FastAPI Backend for Personalized Book Recommendation system\n",
    "\n",
    "This FastAPI application serves as a backend for a Book Recommendation System. It provides several endpoints to retrieve book recommendations based on user preferences or similar books.\n",
    "\n",
    "## Code Breakdown\n",
    "\n",
    "### Imports\n",
    "\n",
    "- `FastAPI`, `HTTPException`: For building the API and handling errors.\n",
    "- `BaseModel`: Used to define Pydantic models for structured data validation.\n",
    "- `List`, `Optional`: For defining types in the models.\n",
    "- `uvicorn`: For running the FastAPI server.\n",
    "- `nest_asyncio`: Allows the FastAPI app to work with nested event loops.\n",
    "- `CORSMiddleware`: Middleware for enabling Cross-Origin Resource Sharing (CORS).\n",
    "\n",
    "### FastAPI Initialization\n",
    "\n",
    "```python\n",
    "app = FastAPI(\n",
    "    title=\"Book Recommendation System API\",\n",
    "    description=\"API for getting book recommendations\",\n",
    "    version=\"1.0.0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa99aba-2122-44f6-8751-fe66750e3e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama\\AppData\\Local\\Temp\\ipykernel_6196\\3815623776.py:54: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "INFO:     Started server process [6196]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded successfully!\n",
      "\n",
      "Loading preprocessed data and models...\n",
      "Loading preprocessed data...\n",
      "✓ Preprocessed data loaded\n",
      "\n",
      "Loading cached models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FastAPI Backend for Book Recommendation System\n",
    "\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "# from get_recommendations import BookRecommender\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Book Recommendation System API\",\n",
    "    description=\"API for getting book recommendations\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Apply asyncio patch for nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add CORS middleware to allow cross-origin requests\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Allow all origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # Allow all methods (GET, POST, etc.)\n",
    "    allow_headers=[\"*\"],  # Allow all headers\n",
    ")\n",
    "\n",
    "# Initialize the recommender system\n",
    "recommender = BookRecommender()\n",
    "\n",
    "# Define the Recommendation Pydantic model for book data\n",
    "class Recommendation(BaseModel):\n",
    "    Title: str\n",
    "    Author: str\n",
    "    Year: int\n",
    "    Publisher: str\n",
    "    Predicted_Rating: Optional[float] = None  # Optional predicted rating\n",
    "    Similarity: Optional[float] = None  # Optional similarity score\n",
    "\n",
    "# Define the response model for user-based recommendations\n",
    "class UserRecommendationResponse(BaseModel):\n",
    "    knn: List[Recommendation]\n",
    "    svd: List[Recommendation]\n",
    "\n",
    "# Define the response model for book-based recommendations\n",
    "class BookRecommendationResponse(BaseModel):\n",
    "    recommendations: List[Recommendation]\n",
    "\n",
    "# Event triggered on startup to load models\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"\"\"Load models on startup\"\"\"\n",
    "    if not recommender.load_models():\n",
    "        raise Exception(\"Failed to load recommendation models\")\n",
    "\n",
    "# Root endpoint for the API\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Book Recommendation System API\"}\n",
    "\n",
    "# Endpoint to get user-based recommendations\n",
    "@app.get(\"/user/{user_id}\", response_model=UserRecommendationResponse)\n",
    "def get_user_recommendations(user_id: int, num_recommendations: int = 10):\n",
    "    \"\"\"Get recommendations for a specific user\"\"\"\n",
    "    try:\n",
    "        # Fetch recommendations for the given user\n",
    "        recommendations = recommender.get_user_recommendations(user_id, num_recommendations)\n",
    "        \n",
    "        # Raise error if no recommendations found\n",
    "        if not recommendations:\n",
    "            raise HTTPException(status_code=404, detail=\"No recommendations found for this user\")\n",
    "        \n",
    "        # Format the recommendations into the response model\n",
    "        formatted_recs = {\n",
    "            'knn': [\n",
    "                Recommendation(\n",
    "                    Title=rec['Title'],\n",
    "                    Author=rec['Author'],\n",
    "                    Year=rec['Year'],\n",
    "                    Publisher=rec['Publisher'],\n",
    "                    Predicted_Rating=rec['Predicted Rating']\n",
    "                ) for rec in recommendations['knn']\n",
    "            ],\n",
    "            'svd': [\n",
    "                Recommendation(\n",
    "                    Title=rec['Title'],\n",
    "                    Author=rec['Author'],\n",
    "                    Year=rec['Year'],\n",
    "                    Publisher=rec['Publisher'],\n",
    "                    Predicted_Rating=rec['Predicted Rating']\n",
    "                ) for rec in recommendations['svd']\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return formatted_recs\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors during the recommendation process\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Endpoint to get book-based recommendations\n",
    "@app.get(\"/book/{book_title}\", response_model=BookRecommendationResponse)\n",
    "def get_book_recommendations(book_title: str, num_recommendations: int = 10):\n",
    "    \"\"\"Get similar book recommendations\"\"\"\n",
    "    try:\n",
    "        # Fetch recommendations for the given book title\n",
    "        recommendations = recommender.get_book_recommendations(book_title, num_recommendations)\n",
    "        \n",
    "        # Raise error if no recommendations found\n",
    "        if not recommendations:\n",
    "            raise HTTPException(status_code=404, detail=\"No recommendations found for this book\")\n",
    "        \n",
    "        # Format the recommendations into the response model\n",
    "        formatted_recs = {\n",
    "            'recommendations': [\n",
    "                Recommendation(\n",
    "                    Title=rec['Title'],\n",
    "                    Author=rec['Author'],\n",
    "                    Year=rec['Year'],\n",
    "                    Publisher=rec['Publisher'],\n",
    "                    Similarity=rec['Similarity']\n",
    "                ) for rec in recommendations\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return formatted_recs\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors during the recommendation process\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Health check endpoint to monitor the API status\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    \"\"\"API health check endpoint\"\"\"\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Function to run the FastAPI server with logging\n",
    "def run_server():\n",
    "    \"\"\"Run the FastAPI server with logging\"\"\"\n",
    "    try:\n",
    "        # Start the FastAPI server using uvicorn\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "    except Exception as e:\n",
    "        # Log the error if server startup fails\n",
    "        logger.error(f\"Server startup failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Entry point to run the server\n",
    "if __name__ == \"__main__\":\n",
    "    run_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
